{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zaimportowanie wymaganych bibliotek\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, Conv2DTranspose\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.losses import binary_crossentropy\n",
    "from numpy import zeros,ones,vstack\n",
    "from numpy.random import randn,randint,normal,random,choice\n",
    "from numpy.random import default_rng\n",
    "from IPython.display import clear_output\n",
    "from math import trunc\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters & data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametry\n",
    "VERSION=1.0\n",
    "\n",
    "#can be cub-cubism or del-delaunay\n",
    "#DATA=\"cubism\"\n",
    "DATA=\"delaunay\"\n",
    "#DATA=\"wikiart\"\n",
    "\n",
    "\n",
    "#Number of saved images during training\n",
    "OUTPUT = 5 # grid of output x output images\n",
    "# For now there is no margin between images\n",
    "SAVE_FREQ = 5 #How often to save images and models\n",
    "OVERRIDE_MODEL = False #Option to either save latest model or all of them\n",
    "OUTPUT_PATH='output\\\\new\\\\' # where to save images\n",
    "STAT_PATH=\"output\\\\statistics\\\\\" # where to save stats\n",
    "'''\n",
    "In this use case, our latent space representations are used to\n",
    "transform more complex forms of raw data (i.e. images, video),\n",
    "into simpler representations which are \"more convenient to process\" and analyze.\n",
    "'''\n",
    "NOISE_SIZE = 128#Lantent dimention size\n",
    "\n",
    "EPOCHS = 5000 #Iterations the biger, the longer the model will train\n",
    "BATCH_SIZE = 10 #number of images in the Batch. Larger Batch → Weak Generalization, Larger Batches → Fewer updates + shifting data → lower computational costs\n",
    "\n",
    "#Columns of stats\n",
    "STATS=[[\"Epoch\",\"batch_t\",\"d_loss\",\"g_loss\",\"acc_real\",\"acc_fake\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA ==\"delaunay\":\n",
    "    DATASET_NAME=\"del\"\n",
    "    PATH_TO_DATA='data\\\\delaunay_data_norm.npy'\n",
    "elif DATA ==\"cubism\":\n",
    "    DATASET_NAME=\"cub\"\n",
    "    PATH_TO_DATA='data\\\\cubism_data_norm.npy'\n",
    "else:\n",
    "    DATASET_NAME=\"wikiart\"\n",
    "    PATH_TO_DATA='data\\\\wikiart_data_norm.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(number of images, size_x, size_y, color_channels)\n",
      "(11503, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "#data load\n",
    "train_data = np.load(PATH_TO_DATA)\n",
    "print(\"(number of images, size_x, size_y, color_channels)\")\n",
    "print(train_data.shape)\n",
    "#checking if the images are square\n",
    "if train_data.shape[1]!=train_data.shape[2]:\n",
    "    print(\"Something is wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of images\n",
    "IMG_SIZE = train_data.shape[1] #rows/cols\n",
    "IMG_CHANNELS = train_data.shape[3] #color channels in our images\n",
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS) #shape of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Image example\n",
    "Image.fromarray(((train_data[1]+1)*255/2).astype(np.uint8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator\n",
    "def build_discriminator(image_shape):\n",
    "    model = Sequential(name=\"Discriminator\")\n",
    "    init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, padding=\"same\",\n",
    "                     input_shape=image_shape, data_format=\"channels_last\", kernel_initializer=init, use_bias=False))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.20))    \n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\", kernel_initializer=init, use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.20))    \n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\",kernel_initializer=init, use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))    \n",
    "    model.add(Dropout(0.20))\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size=4, strides=2, padding=\"same\",kernel_initializer=init, use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))    \n",
    "    model.add(Dropout(0.20))\n",
    "    \n",
    "    model.add(Conv2D(1024, kernel_size=4, strides=2, padding=\"same\",kernel_initializer=init, use_bias=False))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))    \n",
    "    model.add(Dropout(0.20))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator\n",
    "def build_generator(noise_size, channels):\n",
    "     model = Sequential(name=\"Generator\")\n",
    "     init = RandomNormal(mean=0.0, stddev=0.02)\n",
    "     model.add(Dense(4 * 4 * 128,input_dim=noise_size,kernel_initializer=init,use_bias=False))\n",
    "     #model.add(Activation(\"relu\"))\n",
    "     model.add(Reshape((4, 4, 128)))\n",
    "     \n",
    "     model.add(Conv2DTranspose(512,kernel_size=4, strides=2, padding=\"same\",kernel_initializer=init,use_bias=False))\n",
    "     model.add(BatchNormalization(momentum=0.8))\n",
    "     model.add(Activation(\"relu\"))\n",
    "     \n",
    "     model.add(Conv2DTranspose(256,kernel_size=4, strides=2, padding=\"same\", kernel_initializer=init, use_bias=False))\n",
    "     model.add(BatchNormalization(momentum=0.8))\n",
    "     model.add(Activation(\"relu\"))    \n",
    "\n",
    "     model.add(Conv2DTranspose(128,kernel_size=4, strides=2, padding=\"same\", kernel_initializer=init, use_bias=False))\n",
    "     model.add(BatchNormalization(momentum=0.8))\n",
    "     model.add(Activation(\"relu\"))\n",
    "     \n",
    "     #model.add(Conv2DTranspose(128,kernel_size=4, strides=2, padding=\"same\", kernel_initializer=init, use_bias=False))\n",
    "     #model.add(BatchNormalization(momentum=0.8))\n",
    "     #model.add(Activation(\"relu\"))\n",
    "     \n",
    "     model.add(Conv2DTranspose(channels, kernel_size=4,strides=2, padding=\"same\", kernel_initializer=init, use_bias=False))\n",
    "     model.add(BatchNormalization(momentum=0.8))\n",
    "     model.add(Activation(\"tanh\"))\n",
    "\n",
    "     model.summary()\n",
    "\n",
    "     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generative adversarial network\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential(name=\"GAN\")\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving examples during training\n",
    "def save_images(epoch,generator, noise):\n",
    "    generated_images = generator.predict(noise)\n",
    "    array=np.empty((IMG_SHAPE),float)\n",
    "    count=0\n",
    "    #creating a grid of images\n",
    "    for i in range(OUTPUT):\n",
    "        for j in range(OUTPUT):\n",
    "            if j==0:\n",
    "                array_pom=np.array(generated_images[count].reshape(64,64,3,order='C'))\n",
    "            else:\n",
    "                array_pom=np.concatenate((array_pom,np.array(\n",
    "                    generated_images[count].reshape(64,64,3,order='C'))),axis=1)\n",
    "            count+=1\n",
    "        if i==0:\n",
    "            array=array_pom\n",
    "        else:\n",
    "            array=np.concatenate((array,array_pom),axis=0)\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "    filename = os.path.join(OUTPUT_PATH, DATA+\"_v_%.1f_e_%d.png\"%(VERSION,epoch))\n",
    "    #from (-1,1) to (0,255)\n",
    "    im = Image.fromarray(((array+1)*255/2).astype(np.uint8))\n",
    "    im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting real samples from the data\n",
    "def get_real_samples(data, n_samples):\n",
    "    indexes = randint(0, data.shape[0], n_samples)\n",
    "    X = data[indexes]\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "#generating fake samples using generator\n",
    "def generate_fake_samples(generator, n_samples, noise):\n",
    "    x_input = default_rng().normal(0.0, 1.0, (n_samples, noise))\n",
    "    X = generator.predict(x_input)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "#creating errors in labels \n",
    "def flip_labels(y, proc_to_flip):\n",
    " # determine the number of labels to flip\n",
    " n_select = int(proc_to_flip * y.shape[0])\n",
    " # choose labels to flip\n",
    " flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n",
    " # invert the labels in place\n",
    " y[flip_ix] = 1 - y[flip_ix]\n",
    " return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graf plotting and model saving\n",
    "def summarize_performance(epoch, generator, stats=STATS):\n",
    "    st=pd.DataFrame(stats[1:], columns=stats[0])\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].set_title(\"Loss\")#Loss\n",
    "    ax[0].plot(st.g_loss, label='generator')#on fake\n",
    "    ax[0].plot(st.d_loss, label='discriminator')#on real\n",
    "    ax[0].legend()\n",
    "    ax[1].set_title(\"Accuracy\")#Accuracy\n",
    "    ax[1].plot(st.acc_fake, label='generator')#fake\n",
    "    ax[1].plot(st.acc_real, label='discriminator')#real\n",
    "    ax[1].legend()\n",
    "    fig.suptitle(\"Epoch: %d\"%(epoch))\n",
    "    fig.tight_layout()\n",
    "    #plt.savefig((STAT_PATH+\"statistics_plot_v_%.1f.png\" % (VERSION)))\n",
    "    plt.show()\n",
    "    st.to_csv((STAT_PATH+'stats_'+DATASET_NAME+'_%.1f.csv'%(VERSION)))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "def save_generator(epoch, generator, override_model=OVERRIDE_MODEL):\n",
    "    if override_model:\n",
    "        filename = DATASET_NAME+'_model_v_%.1f.h5' % (VERSION)\n",
    "    else:\n",
    "        filename = DATASET_NAME+'_model_v_%.1f_e_%d.h5' % (VERSION,epoch)\n",
    "    generator.save('models\\\\new\\\\'+filename,include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, gan_model, dataset, \n",
    "          noise=NOISE_SIZE, n_epochs=1000, n_batch=10):\n",
    "    batchs_per_epoch = int(dataset.shape[0] / n_batch)\n",
    "    batch_size_r=int(int(n_batch / 2))\n",
    "    #generating fixed noise for images examples\n",
    "    fixed_noise = default_rng().normal(\n",
    "        loc=0.0, scale=1.0, size=(OUTPUT * OUTPUT, noise))\n",
    "    i=0\n",
    "    while i <= n_epochs:\n",
    "        for j in range(batchs_per_epoch):\n",
    "            #geting and generating samples\n",
    "            X_real, y_real = get_real_samples(dataset, batch_size_r)\n",
    "            X_fake, y_fake = generate_fake_samples(\n",
    "                generator, n_batch-batch_size_r,  noise)\n",
    "            X_gan = default_rng().normal(0.0, 1.0, (n_batch, noise))\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            #creating errors in labels\n",
    "            #y_real =flip_labels(y_real, 0.05)\n",
    "            #y_fake =flip_labels(y_fake, 0.05)\n",
    "            #stacking real and false images for traning\n",
    "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "            #training\n",
    "            d_loss, acc_mix = discriminator.train_on_batch(X, y)\n",
    "            _, acc_real = discriminator.evaluate(X_real, y_real, verbose=0)\n",
    "            _, acc_fake = discriminator.evaluate(X_fake, y_fake, verbose=0)\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            STATS.append([i+1,(i)*batchs_per_epoch+(j+1), \n",
    "                          d_loss, g_loss, acc_real, acc_fake])\n",
    "            print('Epoch:%d, Batch:%d/%d, d_loss=%.4f g_loss=%.4f' %\n",
    "                  (i+1, j+1, batchs_per_epoch, d_loss, g_loss))\n",
    "        if (i+1) % SAVE_FREQ == 0:\n",
    "            #saving images generated from fixed noise\n",
    "            save_images(i+1,generator, fixed_noise)\n",
    "            save_generator(i+1,generator)\n",
    "            clear_output()\n",
    "            summarize_performance(i+1,STATS)\n",
    "            print('Accuracy on real: %.0f%%, on fake: %.0f%%' % \n",
    "                  (acc_real*100, acc_fake*100))\n",
    "            #increase the real batch part in discriminator training\n",
    "            #with each save add 2,5%\n",
    "            if batch_size_r<n_batch:\n",
    "                batch_size_r+=int(int(n_batch / 80))            \n",
    "        i+=1\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_142 (Conv2D)         (None, 32, 32, 64)        3072      \n",
      "                                                                 \n",
      " leaky_re_lu_135 (LeakyReLU)  (None, 32, 32, 64)       0         \n",
      "                                                                 \n",
      " dropout_133 (Dropout)       (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_143 (Conv2D)         (None, 16, 16, 128)       131072    \n",
      "                                                                 \n",
      " batch_normalization_224 (Ba  (None, 16, 16, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_136 (LeakyReLU)  (None, 16, 16, 128)      0         \n",
      "                                                                 \n",
      " dropout_134 (Dropout)       (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_144 (Conv2D)         (None, 8, 8, 256)         524288    \n",
      "                                                                 \n",
      " batch_normalization_225 (Ba  (None, 8, 8, 256)        1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_137 (LeakyReLU)  (None, 8, 8, 256)        0         \n",
      "                                                                 \n",
      " dropout_135 (Dropout)       (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_145 (Conv2D)         (None, 4, 4, 512)         2097152   \n",
      "                                                                 \n",
      " batch_normalization_226 (Ba  (None, 4, 4, 512)        2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_138 (LeakyReLU)  (None, 4, 4, 512)        0         \n",
      "                                                                 \n",
      " dropout_136 (Dropout)       (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " conv2d_146 (Conv2D)         (None, 2, 2, 1024)        8388608   \n",
      "                                                                 \n",
      " batch_normalization_227 (Ba  (None, 2, 2, 1024)       4096      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_139 (LeakyReLU)  (None, 2, 2, 1024)       0         \n",
      "                                                                 \n",
      " dropout_137 (Dropout)       (None, 2, 2, 1024)        0         \n",
      "                                                                 \n",
      " flatten_30 (Flatten)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,155,969\n",
      "Trainable params: 11,152,129\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n",
      "Model: \"Generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_61 (Dense)            (None, 2048)              262144    \n",
      "                                                                 \n",
      " reshape_30 (Reshape)        (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_137 (Conv2  (None, 8, 8, 512)        1048576   \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_228 (Ba  (None, 8, 8, 512)        2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_140 (Activation)  (None, 8, 8, 512)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_138 (Conv2  (None, 16, 16, 256)      2097152   \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_229 (Ba  (None, 16, 16, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_141 (Activation)  (None, 16, 16, 256)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_139 (Conv2  (None, 32, 32, 128)      524288    \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_230 (Ba  (None, 32, 32, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_142 (Activation)  (None, 32, 32, 128)      0         \n",
      "                                                                 \n",
      " conv2d_transpose_140 (Conv2  (None, 64, 64, 3)        6144      \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_231 (Ba  (None, 64, 64, 3)        12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_143 (Activation)  (None, 64, 64, 3)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,941,900\n",
      "Trainable params: 3,940,102\n",
      "Non-trainable params: 1,798\n",
      "_________________________________________________________________\n",
      "Model: \"GAN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Generator (Sequential)      (None, 64, 64, 3)         3941900   \n",
      "                                                                 \n",
      " Discriminator (Sequential)  (None, 1)                 11155969  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,097,869\n",
      "Trainable params: 3,940,102\n",
      "Non-trainable params: 11,157,767\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "Epoch:1, Batch:1/1150, d_loss=0.8992 g_loss=0.6695\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:2/1150, d_loss=0.1843 g_loss=0.8254\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:3/1150, d_loss=0.0057 g_loss=0.9830\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:4/1150, d_loss=0.0978 g_loss=1.0080\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:5/1150, d_loss=0.0033 g_loss=0.9913\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:6/1150, d_loss=0.0065 g_loss=1.0422\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:7/1150, d_loss=0.0395 g_loss=0.9723\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch:1, Batch:8/1150, d_loss=0.1348 g_loss=1.4136\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:9/1150, d_loss=0.3163 g_loss=2.4239\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch:1, Batch:10/1150, d_loss=0.0075 g_loss=3.2665\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:11/1150, d_loss=0.0870 g_loss=3.9868\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:12/1150, d_loss=0.0725 g_loss=5.0262\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:13/1150, d_loss=0.0042 g_loss=5.1979\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "Epoch:1, Batch:14/1150, d_loss=0.0485 g_loss=6.6126\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch:1, Batch:15/1150, d_loss=0.0064 g_loss=7.0570\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:16/1150, d_loss=0.6348 g_loss=11.4260\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:17/1150, d_loss=0.0001 g_loss=12.1173\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:18/1150, d_loss=0.0015 g_loss=10.3429\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch:1, Batch:19/1150, d_loss=0.0059 g_loss=6.9713\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch:1, Batch:20/1150, d_loss=0.3476 g_loss=13.6839\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch:1, Batch:21/1150, d_loss=0.0000 g_loss=15.4698\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:22/1150, d_loss=0.0000 g_loss=13.4921\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch:1, Batch:23/1150, d_loss=0.0000 g_loss=8.7134\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:24/1150, d_loss=0.0034 g_loss=3.1404\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:25/1150, d_loss=0.0518 g_loss=8.3207\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:26/1150, d_loss=0.0011 g_loss=7.4652\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:27/1150, d_loss=0.1707 g_loss=16.8637\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:28/1150, d_loss=0.3638 g_loss=19.3814\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:29/1150, d_loss=0.0000 g_loss=15.5655\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:30/1150, d_loss=0.0109 g_loss=8.1256\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:31/1150, d_loss=3.4097 g_loss=29.7608\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:32/1150, d_loss=0.0000 g_loss=29.8067\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:33/1150, d_loss=0.0000 g_loss=23.3397\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "Epoch:1, Batch:34/1150, d_loss=0.0000 g_loss=17.3829\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:35/1150, d_loss=0.0008 g_loss=8.5245\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Epoch:1, Batch:36/1150, d_loss=0.1102 g_loss=9.3329\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch:1, Batch:37/1150, d_loss=0.0702 g_loss=14.7525\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:38/1150, d_loss=0.0064 g_loss=11.2893\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:39/1150, d_loss=1.2024 g_loss=39.3724\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:40/1150, d_loss=0.2854 g_loss=41.6372\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:41/1150, d_loss=0.6246 g_loss=38.9994\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:42/1150, d_loss=0.0000 g_loss=24.6274\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:43/1150, d_loss=0.0000 g_loss=11.1295\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch:1, Batch:44/1150, d_loss=0.0005 g_loss=0.0351\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch:1, Batch:45/1150, d_loss=0.0003 g_loss=0.0001\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:46/1150, d_loss=0.1714 g_loss=11.1584\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:47/1150, d_loss=0.0012 g_loss=10.4906\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Epoch:1, Batch:48/1150, d_loss=0.0007 g_loss=0.6155\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:49/1150, d_loss=0.0781 g_loss=3.3377\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch:1, Batch:50/1150, d_loss=2.1424 g_loss=36.9928\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch:1, Batch:51/1150, d_loss=0.0000 g_loss=38.1270\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Epoch:1, Batch:52/1150, d_loss=0.0000 g_loss=31.6853\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:53/1150, d_loss=0.0000 g_loss=25.1128\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:54/1150, d_loss=0.0000 g_loss=19.8915\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Epoch:1, Batch:55/1150, d_loss=0.0000 g_loss=15.2092\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch:1, Batch:56/1150, d_loss=0.0000 g_loss=10.2351\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Epoch:1, Batch:57/1150, d_loss=0.0032 g_loss=3.7976\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch:1, Batch:58/1150, d_loss=0.0025 g_loss=0.0106\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch:1, Batch:59/1150, d_loss=1.8125 g_loss=28.7622\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch:1, Batch:60/1150, d_loss=0.0000 g_loss=31.9915\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Epoch:1, Batch:61/1150, d_loss=0.0000 g_loss=28.7390\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch:1, Batch:62/1150, d_loss=0.0000 g_loss=24.2998\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch:1, Batch:63/1150, d_loss=0.0000 g_loss=19.4291\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch:1, Batch:64/1150, d_loss=0.0001 g_loss=14.9053\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Epoch:1, Batch:65/1150, d_loss=0.0000 g_loss=9.7437\n",
      "1/1 [==============================] - 0s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "#run the training\n",
    "discriminator = build_discriminator(IMG_SHAPE)\n",
    "generator = build_generator(NOISE_SIZE, IMG_CHANNELS)\n",
    "STATS=[STATS[0]]\n",
    "gan_model = build_gan(generator, discriminator)\n",
    "train(generator, discriminator, gan_model, np.array(train_data), NOISE_SIZE,EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "col = STATS[0]\n",
    "df = pd.read_csv(\"output\\\\statistics\\\\stats_cub_0.8.csv\", usecols=col, sep=',',engine='python')\n",
    "#print(\"Contents in csv file:\", df)\n",
    "plt.plot(df.acc_real, label='Dyskryminator')\n",
    "plt.plot(df.acc_fake, label='Generator')\n",
    "plt.title(\"Accuracy during traning\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "col = STATS[0]\n",
    "df = pd.read_csv(\"output\\\\statistics\\\\stats_cub_0.8.csv\", usecols=col,sep=',',engine='python')\n",
    "#print(\"Contents in csv file:\", df)\n",
    "plt.plot(df.g_loss, color='g', label='Generator')\n",
    "plt.plot(df.d_loss, color='b', label='Dyskryminator')\n",
    "plt.title(\"Loss during traning\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating image grid on already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_model(filename=\"\"):\n",
    "    generator=Sequential()\n",
    "    if(filename!=\"\"):\n",
    "        generator=load_model(\"models\\\\\"+filename,compile=False)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_image_grid(generator, x=1, y=1):\n",
    "    latent_points = default_rng().normal(0.0, 1.0, (x*y, NOISE_SIZE))\n",
    "    X = generator.predict(latent_points)\n",
    "    array=np.empty(((IMG_SHAPE)),float)\n",
    "    count=0\n",
    "    for i in range(y):\n",
    "        for j in range(x):\n",
    "            if j==0:\n",
    "                array_pom=np.array(X[count].reshape(64, 64, 3, order='C'))\n",
    "            else:\n",
    "                array_pom=np.concatenate((\n",
    "                    array_pom,np.array(X[count].reshape(\n",
    "                        64, 64, 3, order='C'))), axis=1)\n",
    "            count+=1\n",
    "        if i==0:\n",
    "            array=array_pom\n",
    "        else:\n",
    "            array=np.concatenate((array, array_pom), axis=0)\n",
    "    array=(array+1)*255/2\n",
    "    return array.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_gen_model(\"examples\\\\cub_model_v_0.8_e_32-.h5\")\n",
    "x=8\n",
    "y=8\n",
    "array = gen_image_grid(model, x, y)\n",
    "image = Image.fromarray((array).astype(np.uint8))\n",
    "image.show()\n",
    "#image.save(\"output\\\\a1.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93d942669e1cf03727884ffd0f90dfde4a7a65cf123d21da6912f8331dcd819b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
